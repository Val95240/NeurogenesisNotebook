{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook by Valentin GUILLET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# On the role of neurogenesis in overcoming catastrophic forgetting\n",
    "*German I. Parisi, Xu Ji, Stefan Wermter, 30 November 2018*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I will present the work of Parisi *et al* and the new network architecture they designed to solve the well-known issue of catastrophic forgetting. I will try to present the results obtained in [this article](https://arxiv.org/abs/1811.02113) presented to NIPS 2018 Workshop on Continual Learning (cf. Bibliography)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I. [Catastrophic forgetting and Neurogenesis](#part1)<br/>\n",
    "II. [GWR Networks](#part2)<br/>\n",
    "1. [The algorithm](#algo)<br/>\n",
    "2. [A fake dataset](#dataset)<br/>\n",
    "3. [Implementation](#implem)<br/>\n",
    "4. [Labeling](#label)<br/>\n",
    "5. [A real dataset](#iris)<br/>\n",
    "\n",
    "III. [Overcoming catastrophic forgetting ?](#part3)<br/>\n",
    "IV. [Deep GWR](#part4)<br/>\n",
    "[Conclusion](#ccl)<br/>\n",
    "[Bibliography](#biblio)<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=part1></a>I. Catastrophic forgetting and Neurogenesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we need to understand one of the inherent issue of Neural Networks that the authors try to solve : **catastrophic forgetting**.\n",
    "\n",
    "For that, let's suppose that we need to solve a classification task with a Deep Neural Network such as recognizing pictures of cats and dogs.\n",
    "With the current available algorithms, this task is not too hard : we define a NN with a predefined architecture, and then we simply train it on a dataset with pictures of the categories we want to recognize (here, cats and dogs).\n",
    "\n",
    "This works really well, but imagine now that after that learning process, we want to also recognize pictures of rabbits.\n",
    "How can we do it ?\n",
    "\n",
    "- One possible way is to train a new network from scratch with a dataset containing both cats, dogs and rabbits pictures. This works well but we need to relearn from scratch each time we discover a new category and the more categories there are, the longer the training will be.\n",
    "\n",
    "\n",
    "- Another way is to reuse the same network and train it for a second time on a new dataset containing only rabbit pictures. During this second learning, as we apply gradient descents, the parameters of the network will move towards a configuration that is able to recognize rabbits, but in consequence they will also move away from the previously learned configuration that was able to distinguish between cats and dogs. That means that when we try to learn a new feature, the network forgets what it previously learned : that is what is called **catastrophic forgetting**\n",
    "\n",
    "To illustrate this second situation, let's imagine that we want to train a robot to move in a house and to interact with its environment. This robot will have to recognize the objects around it in order to take the right actions, but it is impossible to establish in advance a list of every object it will ever see during its life ! As we can't make a dataset with every objects it will see, the only solution we have is to make the robot able to learn continuously while it interacts with the environment : this problematic is called **lifelong learning**.\n",
    "\n",
    "In summary, the final goal is to make an algorithm able to learn continuously as it gets new experience (**lifelong learning**) while avoiding to overwrite previously learned features (**catastrophic forgetting**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One interesting approach to solve this difficult task is to get inspired by biology : indeed, the human brain is to a certain extent similar to Neural Networks so it must face the same problematics and, as you didn't forget what you learned during highschool when you arrived in Supaero (well, at least not everything), that means that nature found a way to avoid catastrophic forgetting.\n",
    "\n",
    "Indeed, based on previous research and many psychological and neurobiological studies, one mechanism that seems important to avoid this is **neurogenesis**, i.e. the generation of new neurons as the brain encounters new situations. The idea is that if no neuron is adapted to the input, then a new neuron needs to be created to fill this gap.\n",
    "Inspired by this idea, in 2002 Stephen Marsland and his team developped a new network architecture that reproduce this mechanism called **Growing When Required**, or **GWR**.\n",
    "\n",
    "One main difference between this architecture and classical Neural Networks is that GWR is based on **competitive learning**, a form of unsupervised learning where neurons, called nodes in this setup, compete for the right to respond to a subset of the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=part2></a>II. GWR Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"algo\"></a>1. The algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I will present the base of Parisi's work : the GWR architecture.\n",
    "\n",
    "A GWR network is composed of two main elements : nodes and edges.\n",
    "A **node** is a point in the input space and the objective of the network is to learn a set of nodes that represent well the distribution of the input space.\n",
    "\n",
    "Nodes that are close together are linked by **edges** to form neighbourhoods of nodes that represent similar perception.\n",
    "Each edge has an *age* that corresponds to the number of iterations since it hasn't been activated.\n",
    "\n",
    "During the learning, the nodes are going to move in the input space to best correspond to the input distribution.\n",
    "When seeing a new sample, we select the closest node to this sample, called the **Best Matching Unit (BMU)** and if the distance between this node and the sample is below a threshold, we move the BMU and its neighbours towards the sample to better represent it.\n",
    "In the case where no node is close enough to the sample, that means that our set of nodes is still distant from the input distribution and we add a new node to our set (it is the **neurogenesis**).\n",
    "\n",
    "Each node has a **firing counter**, initialized to 1 and that can only decrease, that indicates if this node fired a lot in its life or not.\n",
    "\n",
    "When we add a new node to our network, we want to be sure to leave him the necessary time to learn the input distribution before adding another node. One way is to give a **firing counter** to each node, initialized to 1 and that decreases each time the associated node is the BMU of a sample. This value indicates the number of times the node was useful to represent the input distribution, and so we add a new neuron only if the firing counter of the BMU is low enough.\n",
    "\n",
    "Each time a node is determined as BMU, its firing counter and the firing counter of each of its neighbour is decreased following $$\\textrm{firing_counter} \\leftarrow \\textrm{firing_counter} + \\tau_i * \\kappa * (1 - \\textrm{firing_counter}) - \\tau_i$$ with $\\kappa$, $\\tau_i = \\tau_{\\mathrm{BMU}}$ for the BMU and $\\tau_i = \\tau_{\\mathrm{neighbour}}$ for the BMU's neighbours three constants.\n",
    "\n",
    "So the learning goes as follow :\n",
    "1. Sample a point $x$ from the input dataset\n",
    "\n",
    "\n",
    "2. For each node $w_i$ in the network, compute the distance between it and the sample $d_i = \\lVert w_i - x \\rVert$\n",
    "\n",
    "\n",
    "3. Select the two nodes that have the smallest distance. The closest one is called Best Matching Unit (BMU) : $$\\mathrm{BMU} = \\underset{i \\in \\mathrm{Nodes}}{\\operatorname{argmin}}{\\lVert w_i - x \\rVert}$$ $$\\mathrm{Second} = \\underset{i \\in \\mathrm{Nodes} / \\{\\mathrm{BMU}\\}}{\\operatorname{argmin}}{\\lVert w_i - x \\rVert}$$\n",
    "\n",
    "\n",
    "4. Create an edge between these two closest nodes if they were not linked before, and reset the age of this edge to zero $\\mathrm{Edge} = \\mathrm{Edge} \\cup \\{(\\mathrm{BMU}, \\mathrm{Second})\\}$\n",
    "\n",
    "\n",
    "5. If $\\exp(-d_{\\mathrm{BMU}}) < \\alpha_t$ and $\\mathrm{BMU}.firing counter < h_t$ (i.e. the BMU is too far from the sample and has been added several time steps before)\n",
    "        a) Add a new node to the network in the middle of the BMU and the sample\n",
    "$$w_{new} = \\frac{w_{\\mathrm{BMU}} + x}{2}$$\n",
    "        b) Connect the new node to the BMU and the second best unit\n",
    "$$\\mathrm{Edge} = (\\mathrm{Edge} / \\{(\\mathrm{BMU}, \\mathrm{Second})\\}) \\cup \\{(\\mathrm{BMU}, \\mathrm{new}), (\\mathrm{Second}, \\mathrm{new})\\}$$\n",
    "\n",
    "\n",
    "    Else:\n",
    "        Move the BMU and its neighbours towards the sample\n",
    "$$w_{\\mathrm{BMU}} += \\epsilon_{\\mathrm{best}} * \\mathrm{BMU}.firing counter * (x - w_{\\mathrm{BMU}})$$\n",
    "$$\\forall neigh \\in neighbours(\\mathrm{BMU}),$$\n",
    "$$w_{\\mathrm{neigh}} += \\epsilon_{\\mathrm{neigh}} * \\mathrm{neigh}.firing counter * (x - w_{\\mathrm{neigh}})$$\n",
    "\n",
    "\n",
    "6. Increase the age of every edge connected to the Best Matching Unit by 1\n",
    "\n",
    "\n",
    "7. Slightly decrease the firing counter of the BMU and its neighbours\n",
    "\n",
    "\n",
    "8. Remove every edge that is older than some constant $\\mathrm{max\\_age}$\n",
    "\n",
    "\n",
    "9. Remove every node that doesn't have any edge\n",
    "\n",
    "with :\n",
    "* $\\alpha_t$ : the activity threshold corresponding to the minimal distance between the BMU and the sample that leads to adding a new node\n",
    "* $h_t$ : the firing threshold corresponding to the maximal firing counter of the BMU that leads to adding a new node\n",
    "* $\\epsilon_{\\mathrm{best}}$ : a coefficient controlling the amplitude of the move of the BMU towards the sample\n",
    "* $\\epsilon_{\\mathrm{neigh}}$ : a coefficient controlling the amplitude of the move of every neighbour of the BMU towards the sample\n",
    "\n",
    "To apply this algorithm, we start by initializing two random nodes in the input space and then the network automatically adjusts these nodes towards the input distribution, and when the existing nodes are not sufficient to describe correctly the data, it automatically creates new nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"dataset\"></a>2. A fake dataset : artificial flowers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be sure to understand, let's code a quick example.\n",
    "Let's imagine a fake dataset of flowers that live in three dimensions (so each flower is represented by three characteristics). Let's suppose that we have 200 flowers and 4 categories of flowers.\n",
    "\n",
    "To simulate that, we're gonna generate 200 random 3D-points around 4 random \"representative flowers\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_dataset(nb_points=200, nb_labels=4, var=0.1):\n",
    "\n",
    "    # First, we select where we want our \"representative flowers\" to be\n",
    "    centers = []\n",
    "    while len(centers) < nb_labels:\n",
    "        new_center = np.random.rand(3)\n",
    "        # We make sure that these \"representative flowers\" are not to similar to be able to distinguish them\n",
    "        if all([np.linalg.norm(center - new_center) > 0.5 for center in centers]):\n",
    "            centers.append(new_center)\n",
    "\n",
    "\n",
    "    # Then, we generate some fake flowers with a normal distribution around these \"representative flowers\"\n",
    "    #   and we label each of them with their category\n",
    "    points = []\n",
    "    labels = []\n",
    "    for i in range(nb_points):\n",
    "        center = centers[i % nb_labels]\n",
    "        point = np.random.normal(center, var)\n",
    "        points.append(point)\n",
    "        labels.append(i % nb_labels)\n",
    "        \n",
    "    idx = np.random.permutation(nb_points)\n",
    "\n",
    "    return np.array(points)[idx], np.array(labels)[idx], np.array(centers)\n",
    "\n",
    "fake_flowers, categories, representative_flowers = generate_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize this dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n"
     ]
    }
   ],
   "source": [
    "%matplotlib\n",
    "\n",
    "def plot_dataset(points, labels, centers=None):\n",
    "\n",
    "    fig = plt.figure(\"Data\", figsize=(20, 20))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Plot every fake flower in our dataset\n",
    "    ax.scatter(points[:, 0], points[:, 1], points[:, 2], c=labels)\n",
    "    \n",
    "    # Plot also the \"representative flowers\"\n",
    "    if centers is not None:\n",
    "        ax.scatter(centers[:, 0], centers[:, 1], centers[:, 2], s=150, c='red')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_dataset(fake_flowers, categories, representative_flowers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have four \"representative flowers\" in big red dots, and 200 flowers that each belongs to one of the categories of flower while still being slightly different from each other. Our objective is, given this dataset without the \"representative flowers\", to find a discrete representation of this space.\n",
    "\n",
    "**NB :** A good representation of this dataset is obviously the four \"representative flowers\" used to generate every other fake flower, so in the end we would like to find something similar to these four points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"implem\"></a>3. Implementation of GWR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let's define the structure of Node.\n",
    "This class is very basic : it contains a vector called `weights` representing the node in the input space initialized randomly, and a firing counter initialized to 1.\n",
    "\n",
    "We also implement three methods : one that, given a sample in the input space, returns the distance between this sample and the node, another to update the weights of the node and the last one to update the firing counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Node:\n",
    "\n",
    "    def __init__(self, input_size, weights=None):\n",
    "        if weights is None:\n",
    "            self.weights = np.random.rand(input_size)\n",
    "        else:\n",
    "            self.weights = weights\n",
    "\n",
    "        self.firing_counter = 1\n",
    "\n",
    "    def compute_distance(self, input_vector):\n",
    "        \"\"\"Return the distance between the node and an input vector.\"\"\"\n",
    "        return np.linalg.norm(self.weights - input_vector)**2\n",
    "    \n",
    "    def update_weights(self, input_vector, bmu=False):\n",
    "        \"\"\"Update the weights of the node depending on whether it is the BMU or a neighbour of the BMU.\"\"\"\n",
    "        epsilon = EPS_BMU if bmu else EPS_NEIGH\n",
    "        self.weights += epsilon * self.firing_counter * (input_vector - self.weights)\n",
    "\n",
    "    def update_firing_counter(self, bmu=False):\n",
    "        \"\"\"Update the firing counter of the node depending on whether it is the BMU or a neighbour of the BMU.\"\"\"\n",
    "        tau = TAU_BMU if bmu else TAU_NEIGH\n",
    "        self.firing_counter += tau * KAPPA * (1 - self.firing_counter) - tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test this class by plotting nodes with our fake dataset\n",
    "\n",
    "# Generate 10 random nodes\n",
    "nodes = np.array([Node(3).weights for i in range(10)])\n",
    "\n",
    "# Plot these points in our input space\n",
    "plot_dataset(fake_flowers, categories, nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's implement the edge class.\n",
    "This class contains :\n",
    "* `edges` : the adjacency matrix of the network (i.e. if `edges[node1][node2]` == 1, it means that node1 and node2 are connected)\n",
    "* `ages` : the age of each of these edges\n",
    "\n",
    "We also implement many simple and useful methods to add or remove an edge between two nodes, update the age of edges of the BMU, get every neighbour of a given node or remove a node from the network.\n",
    "\n",
    "**NB :** the adjacency matrix is by definition symetric, we could store it as a triangular matrix for performance reasons but this would complicate the code for just a small gain. Instead, we define two methods `set_edge` and `set_age` to modify the values in the matrix symetrically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Edges:\n",
    "\n",
    "    def __init__(self, nb_nodes):\n",
    "        self.edges = np.zeros((nb_nodes, nb_nodes))\n",
    "        self.ages = np.zeros((nb_nodes, nb_nodes))\n",
    "\n",
    "    def set_edge(self, index1, index2, value):\n",
    "        self.edges[index1, index2] = value\n",
    "        self.edges[index2, index1] = value\n",
    "\n",
    "    def set_age(self, index1, index2, value):\n",
    "        self.ages[index1, index2] = value\n",
    "        self.ages[index2, index1] = value\n",
    "        \n",
    "    def reset_edge(self, best_index, second_index):\n",
    "        \"\"\"Sets an edge between the BMU and the Second BMU with an age of 0\"\"\"\n",
    "        self.set_edge(best_index, second_index, 1)\n",
    "        self.set_age(best_index, second_index, 0)\n",
    "\n",
    "    def add_edge(self, best_index, second_index):\n",
    "        \"\"\"Add a new column to the edge matrix (in case of a new node) and links the new\n",
    "        node to the BMU and the Second BMU\"\"\"\n",
    "        n = self.edges.shape[0]\n",
    "        # Add new columns to edges and ages\n",
    "        tmp = self.edges\n",
    "        self.edges = np.zeros((n+1, n+1))\n",
    "        self.edges[:-1, :-1] = tmp\n",
    "        tmp = self.ages\n",
    "        self.ages = np.zeros((n+1, n+1))\n",
    "        self.ages[:-1, :-1] = tmp\n",
    "\n",
    "        # Remove the link between the BMU and the Second BMU\n",
    "        self.set_edge(best_index, second_index, 0)\n",
    "        self.set_age(best_index, second_index, 0)\n",
    "        \n",
    "        # Link the BMU and the new node, and the Second BMU to the new node\n",
    "        self.set_edge(best_index, n, 1)\n",
    "        self.set_edge(second_index, n, 1)\n",
    "        \n",
    "    def get_neighbours(self, node_index):\n",
    "        \"\"\"Return all the neighbours of a given node\"\"\"\n",
    "        return np.nonzero(self.edges[node_index])[0]\n",
    "        \n",
    "    def update_age(self, best):\n",
    "        \"\"\"Update the age of every edge linked to the BMU\"\"\"\n",
    "        neighbours = self.get_neighbours(best)\n",
    "        for neighbour in neighbours:\n",
    "            age = self.ages[best, neighbour]\n",
    "            self.set_age(best, neighbour, age+1)\n",
    "\n",
    "    def remove_edges(self):\n",
    "        \"\"\"Remove every edge that is older than a threshold\"\"\"\n",
    "        for i in range(self.edges.shape[0]):\n",
    "            for j in range(i+1, self.edges.shape[0]):\n",
    "                if self.ages[i, j] > MAX_AGE:\n",
    "                    self.set_edge(i, j, 0)\n",
    "                    self.set_age(i, j, 0)\n",
    "\n",
    "    def remove_node(self, node_index):\n",
    "        \"\"\"Remove a column and a line from edges and ages corresponding to the node\"\"\"\n",
    "        self.edges = np.delete(self.edges, node_index, axis=0)\n",
    "        self.edges = np.delete(self.edges, node_index, axis=1)\n",
    "        self.ages = np.delete(self.ages, node_index, axis=0)\n",
    "        self.ages = np.delete(self.ages, node_index, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the two simple data structures we need for the GWR algorithm.\n",
    "What we need to do now is simply to define our hyperparameters and to follow the algorithm step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA_T = 0.70     # Insert threshold\n",
    "H_T     = 0.10     # Firing threshold\n",
    "\n",
    "EPS_BMU   = 1      # BMU learning rate\n",
    "EPS_NEIGH = 0.05   # BMU's neighbours learning rate\n",
    "\n",
    "KAPPA     = 1.05   # General firing counter update constant\n",
    "TAU_BMU   = 0.3    # BMU firing counter update constant\n",
    "TAU_NEIGH = 0.1    # BMU's neighbours firing counter update constant\n",
    "\n",
    "MAX_AGE = 25       # Maximum age of an edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Network:\n",
    "\n",
    "    def __init__(self, input_size):\n",
    "        self.input_size = input_size\n",
    "\n",
    "        # We start with only two nodes in our network\n",
    "        self.nb_nodes = 2\n",
    "        self.nodes = []\n",
    "        for i in range(self.nb_nodes):\n",
    "            self.nodes.append(Node(input_size))\n",
    "\n",
    "        self.edges = Edges(self.nb_nodes)\n",
    "\n",
    "    def find_bmus(self, input_vector):\n",
    "        \"\"\"Return the index of the two nodes which are the closest to the input vector (the BMU and the second BMU)\n",
    "        and the distance BMU-input vector.\"\"\"\n",
    "        distance = np.array([node.compute_distance(input_vector)\n",
    "                             for node in self.nodes])\n",
    "        indexes = np.argsort(distance)\n",
    "        return indexes[0], indexes[1], distance[indexes[0]]\n",
    "\n",
    "    def add_node(self, best_index, second_index, input_vector):\n",
    "        \"\"\"Add a new node linked to the BMU and the Second BMU, and located in the middle\n",
    "        of the BMU and the input vector.\"\"\"\n",
    "        new_weights = (input_vector + self.nodes[best_index].weights) / 2\n",
    "        new_node = Node(self.input_size, new_weights)\n",
    "        self.nodes.append(new_node)\n",
    "        self.edges.add_edge(best_index, second_index)\n",
    "        self.nb_nodes += 1\n",
    "\n",
    "    def remove_nodes(self):\n",
    "        \"\"\"Remove a node and the corresponding edges\"\"\"\n",
    "        for node in range(self.nb_nodes-1, -1, -1):\n",
    "            if len(self.edges.get_neighbours(node)) == 0:   # if the node doesn't have any neighbour\n",
    "                self.edges.remove_node(node)\n",
    "                self.nodes.pop(node)\n",
    "                self.nb_nodes -= 1\n",
    "\n",
    "    def fit(self, samples, epochs=10):\n",
    "        \"\"\"Apply the GWR algorithm and return a list of the evolution of the nodes in the input space\n",
    "        and the index of the BMU at each step\"\"\"\n",
    "\n",
    "        weights, indexes = [], []\n",
    "        for epoch in range(epochs):\n",
    "            for sample in samples:\n",
    "\n",
    "                # Step 1 : Find the BMU and the second BMU\n",
    "                bmu, second, bmu_dist = self.find_bmus(sample)\n",
    "\n",
    "                # Step 2 : Add an edge between the BMU and the second and set its age to 0\n",
    "                self.edges.reset_edge(bmu, second)\n",
    "\n",
    "                # Step 3 : If the bmu is too far from the sample and it has already fired a lot in its life :\n",
    "                if np.exp(-bmu_dist) < ALPHA_T and self.nodes[bmu].firing_counter < H_T:\n",
    "                    # Then add a new node in the network\n",
    "                    self.add_node(bmu, second, sample)\n",
    "\n",
    "                else:\n",
    "                    # Else, just move the BMU and its neighbours closer to the sample\n",
    "                    self.nodes[bmu].update_weights(sample, bmu=True)\n",
    "                    for neighbour in self.edges.get_neighbours(bmu):\n",
    "                        self.nodes[neighbour].update_weights(sample)\n",
    "\n",
    "                # Step 4 : update the firing counter of the BMU and of its neighbours\n",
    "                self.nodes[bmu].update_firing_counter(bmu=True)\n",
    "                for neighbour in self.edges.get_neighbours(bmu):\n",
    "                    self.nodes[neighbour].update_firing_counter()\n",
    "\n",
    "                # Step 5 : each edge connected to the BMU gets older\n",
    "                self.edges.update_age(bmu)\n",
    "                # Step 6 : if an edge is too old, remove it\n",
    "                self.edges.remove_edges()\n",
    "                # Step 7 : if a neuron doesn't have any edges, remove it\n",
    "                self.remove_nodes()\n",
    "\n",
    "                weights.append([node.weights.copy() for node in self.nodes])\n",
    "                indexes.append(bmu)\n",
    "\n",
    "        return weights, indexes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just need to instance this network and make it run :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final number of nodes : 4\n"
     ]
    }
   ],
   "source": [
    "input_size = fake_flowers.shape[1]\n",
    "network = Network(input_size)\n",
    "\n",
    "weights, indexes = network.fit(fake_flowers, 1)\n",
    "print(f\"Final number of nodes : {network.nb_nodes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a small function to observe what the algorithm is really doing :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def animate(points, labels, weights, indexes):\n",
    "    \n",
    "    centers = weights[:]\n",
    "    maxi_len = len(max(centers, key=len))\n",
    "    for i in range(len(centers)):\n",
    "        while len(centers[i]) < maxi_len:\n",
    "            centers[i].append(centers[i][-1])\n",
    "        centers[i] = np.array(centers[i])\n",
    "    centers = np.array(centers)\n",
    "    size = centers.shape[1]\n",
    "\n",
    "    def update(num, nodes):\n",
    "        nodes._offsets3d = (centers[num][:, 0], centers[num][:, 1], centers[num][:, 2])\n",
    "        col = ['blue'] * size\n",
    "        col[indexes[num]] = 'red'\n",
    "        nodes._facecolor3d = col\n",
    "        return nodes,\n",
    "\n",
    "    fig = plt.figure(\"Animation\")\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(points[:, 0], points[:, 1], points[:, 2], c=labels)\n",
    "    nodes = ax.scatter(centers[0][:, 0], centers[0][:, 1], centers[0][:, 2], s=200, c='blue')\n",
    "    points_anim = animation.FuncAnimation(fig, update, len(centers), fargs=(nodes,),\n",
    "                                          interval=1, repeat=False)\n",
    "    return fig, points_anim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, _ = animate(fake_flowers, categories, weights, indexes)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each time step, we display the position of the nodes in the input space with a big dot which is blue except for the BMU where it is red.\n",
    "\n",
    "We can see that in the beginning, there are only two nodes that quickly moves toward a flower category. But as there are not enough nodes for each category, the distance between a node and a random sample quickly becomes big, so the network creates a new node. At the end of the training, the network has 4 nodes which are in the center of each of the fake flower distributions, so it is a good representation of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"label\"></a>4. Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm works well but in practice, we want to be able to categorize the data with it. Fortunately, this requires only a very simple modification of the basic algorithm : the addition of a label matrix.\n",
    "\n",
    "The idea is simple : each node stores the number of times that a given sample label $l$ has been associated to its neural weight (i.e. it has been designated BMU).\n",
    "So initially, each node store a frequency of 1 for all the label that exist, and then if a node is designated BMU for a sample with label $l$, it will increase its frequency associated to $l$ and decrease the other frequency.\n",
    "\n",
    "In the end, each node stores the frequency of each label, and so we simply associate the node with the label with the biggest frequency.\n",
    "From that, to categorize a new input data point, we simply compute the BMU of this sample and the prediction is the label of this BMU.\n",
    "\n",
    "Implementation wise, we add an attribute `labels` to the node which is a dictionnary associating each label to its frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LabelizedNode:\n",
    "\n",
    "    def __init__(self, input_size, weights=None):\n",
    "        if weights is None:\n",
    "            self.weights = np.random.rand(input_size)\n",
    "        else:\n",
    "            self.weights = weights\n",
    "\n",
    "        self.labels = {}\n",
    "        self.firing_counter = 1\n",
    "\n",
    "    def compute_distance(self, input_vector):\n",
    "        \"\"\"Return the distance between the node and an input vector.\"\"\"\n",
    "        return np.linalg.norm(self.weights - input_vector)**2\n",
    "    \n",
    "    def update_weights(self, input_vector, bmu=False):\n",
    "        \"\"\"Update the weights of the node depending on whether it is the BMU or a neighbour of the BMU.\"\"\"\n",
    "        epsilon = EPS_BMU if bmu else EPS_NEIGH\n",
    "        self.weights += epsilon * self.firing_counter * (input_vector - self.weights)\n",
    "\n",
    "    def update_firing_counter(self, bmu=False):\n",
    "        \"\"\"Update the firing counter of the node depending on whether it is the BMU or a neighbour of the BMU.\"\"\"\n",
    "        tau = TAU_BMU if bmu else TAU_NEIGH\n",
    "        self.firing_counter += tau * KAPPA * (1 - self.firing_counter) - tau\n",
    "\n",
    "    def add_label(self, label):\n",
    "        \"\"\"Add a new label that we never saw before and initialize its frequency to 1.\"\"\"\n",
    "        self.labels[label] = 1\n",
    "\n",
    "    def copy_labels(self, labels):\n",
    "        \"\"\"Initialize the label dictionnary with every label that has been encountered before.\"\"\"\n",
    "        for key in labels.keys():\n",
    "            self.labels[key] = 1\n",
    "\n",
    "    def update_labels(self, label):\n",
    "        \"\"\"Update the frequency of the label that this node has been associated with.\"\"\"\n",
    "        self.labels[label] += 1\n",
    "        for l in self.labels.keys():\n",
    "            if l != label:\n",
    "                self.labels[l] -= 0.1\n",
    "\n",
    "    def get_label(self):\n",
    "        \"\"\"Get the label with the largest frequency : it is the label associated with this node.\"\"\"\n",
    "        return max(self.labels.keys(), key=lambda k:self.labels[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LabelizedNetwork:\n",
    "\n",
    "    def __init__(self, input_size):\n",
    "        self.input_size = input_size\n",
    "\n",
    "        # We start with only two nodes in our network\n",
    "        self.nb_nodes = 2\n",
    "        self.nodes = []\n",
    "        for i in range(self.nb_nodes):\n",
    "            self.nodes.append(LabelizedNode(input_size))\n",
    "\n",
    "        self.edges = Edges(self.nb_nodes)\n",
    "\n",
    "    def find_bmus(self, input_vector):\n",
    "        \"\"\"Return the index of the two nodes which are the closest to the input vector (the BMU and the second BMU)\n",
    "        and the distance BMU-input vector.\"\"\"\n",
    "        distance = np.array([node.compute_distance(input_vector)\n",
    "                             for node in self.nodes])\n",
    "        indexes = np.argsort(distance)\n",
    "        return indexes[0], indexes[1], distance[indexes[0]]\n",
    "\n",
    "    def add_node(self, best_index, second_index, input_vector, label):\n",
    "        \"\"\"Add a new node linked to the BMU and the Second BMU, and located in the middle\n",
    "        of the BMU and the input vector.\"\"\"\n",
    "        new_weights = (input_vector + self.nodes[best_index].weights) / 2\n",
    "        new_node = LabelizedNode(self.input_size, new_weights)\n",
    "        # We initialize the labels of the new node to 1\n",
    "        new_node.copy_labels(self.nodes[0].labels)\n",
    "        # We increase the frequency of the label associated to the new node\n",
    "        new_node.labels[label] += 1\n",
    "        self.nodes.append(new_node)\n",
    "        self.edges.add_edge(best_index, second_index)\n",
    "        self.nb_nodes += 1\n",
    "\n",
    "    def remove_nodes(self):\n",
    "        \"\"\"Remove a node and the corresponding edges\"\"\"\n",
    "        for node in range(self.nb_nodes-1, -1, -1):\n",
    "            if len(self.edges.get_neighbours(node)) == 0:   # if the node doesn't have any neighbour\n",
    "                self.edges.remove_node(node)\n",
    "                self.nodes.pop(node)\n",
    "                self.nb_nodes -= 1\n",
    "\n",
    "    def fit(self, samples, labels, epochs=10):\n",
    "        \"\"\"Apply the GWR algorithm and return a list of the evolution of the nodes in the input space\n",
    "        and the index of the BMU at each step\"\"\"\n",
    "\n",
    "        weights, indexes = [], []\n",
    "        for epoch in range(epochs):\n",
    "            for sample, label in zip(samples, labels):\n",
    "                \n",
    "                # If the label has never been seen before, we add it to each node with a frequency of 1\n",
    "                if label not in self.nodes[0].labels:\n",
    "                    for node in self.nodes:\n",
    "                        node.add_label(label)\n",
    "\n",
    "                # Step 1 : Find the BMU and the second BMU\n",
    "                bmu, second, bmu_dist = self.find_bmus(sample)\n",
    "\n",
    "                # Step 2 : Add an edge between the BMU and the second and set its age to 0\n",
    "                self.edges.reset_edge(bmu, second)\n",
    "\n",
    "                # Step 3 : If the bmu is too far from the sample and it has already fired a lot in its life :\n",
    "                if np.exp(-bmu_dist) < ALPHA_T and self.nodes[bmu].firing_counter < H_T:\n",
    "                    # Then add a new node in the network\n",
    "                    self.add_node(bmu, second, sample, label)\n",
    "\n",
    "                else:\n",
    "                    # Else, just move the BMU and its neighbours closer to the sample\n",
    "                    self.nodes[bmu].update_weights(sample, bmu=True)\n",
    "                    for neighbour in self.edges.get_neighbours(bmu):\n",
    "                        self.nodes[neighbour].update_weights(sample)\n",
    "                    self.nodes[bmu].update_labels(label)\n",
    "\n",
    "                # Step 4 : update the firing counter of the BMU and of its neighbours\n",
    "                self.nodes[bmu].update_firing_counter(bmu=True)\n",
    "                for neighbour in self.edges.get_neighbours(bmu):\n",
    "                    self.nodes[neighbour].update_firing_counter()\n",
    "\n",
    "                # Step 5 : each edge connected to the BMU gets older\n",
    "                self.edges.update_age(bmu)\n",
    "                # Step 6 : if an edge is too old, remove it\n",
    "                self.edges.remove_edges()\n",
    "                # Step 7 : if a neuron doesn't have any edges, remove it\n",
    "                self.remove_nodes()\n",
    "\n",
    "                weights.append([node.weights.copy() for node in self.nodes])\n",
    "                indexes.append(bmu)\n",
    "\n",
    "        return weights, indexes\n",
    "    \n",
    "    def predict(self, samples):\n",
    "                \n",
    "        labels = []\n",
    "        for sample in samples:\n",
    "            bmu, _, _ = self.find_bmus(sample)\n",
    "            labels.append(self.nodes[bmu].get_label())\n",
    "        return np.array(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now test if the entire algorithm is working but first learning the input distribution on a training dataset and then make some prediction on a testing dataset and compute the generalization error :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_GWR(dataset, labels, disp_anim=True, epochs=1, network=None):\n",
    "    n, input_size = dataset.shape\n",
    "\n",
    "    train_data, train_labels = dataset[:2*n//3], labels[:2*n//3]\n",
    "    test_data, test_labels = dataset[2*n//3:], labels[2*n//3:]\n",
    "\n",
    "    if network is None:\n",
    "        network = LabelizedNetwork(input_size)\n",
    "\n",
    "    weights, indexes = network.fit(train_data, train_labels, epochs)\n",
    "    print(f\"Number of nodes : {network.nb_nodes}\")\n",
    "\n",
    "    train_err = 100 - (network.predict(train_data) == train_labels).sum() / (2 * n // 3) * 100\n",
    "    test_err  = 100 - (network.predict(test_data ) == test_labels ).sum() / (n // 3) * 100\n",
    "    print(f\"Train error : {train_err:.4}%\\n\"\n",
    "          f\"Generalization error : {test_err:.4}%\")\n",
    "\n",
    "    print(\"Confusion matrix :\\n\", confusion_matrix(test_labels, network.predict(test_data)))\n",
    "\n",
    "    if disp_anim:\n",
    "        return animate(train_data, train_labels, weights, indexes)\n",
    "    \n",
    "    else:\n",
    "        return network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes : 4\n",
      "Train error : 0.0%\n",
      "Generalization error : 1.515%\n",
      "Confusion matrix :\n",
      " [[18  0  0  0]\n",
      " [ 0 16  0  0]\n",
      " [ 2  0 15  0]\n",
      " [ 0  0  0 16]]\n"
     ]
    }
   ],
   "source": [
    "fig, _ = test_GWR(fake_flowers, categories)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that each node represents very well a type of flower and that the labeling is very easy because of the non-overlapping categories.\n",
    "Of course, these data were simple but we can make the training more difficult :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_flowers2, categories2, representative_flowers2 = generate_dataset(nb_points=1000, nb_labels=7, var=0.16)\n",
    "plot_dataset(fake_flowers2, categories2, representative_flowers2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes : 10\n",
      "Train error : 9.459%\n",
      "Generalization error : 9.009%\n",
      "Confusion matrix :\n",
      " [[45  0  1  1  0  1  0]\n",
      " [ 1 46  0  0  0  0  5]\n",
      " [ 4  1 44  0  0  0  0]\n",
      " [ 1  0  1 41  0  3  0]\n",
      " [ 0  0  0  0 38  7  0]\n",
      " [ 1  0  0  0  1 45  0]\n",
      " [ 0  3  0  0  0  0 44]]\n"
     ]
    }
   ],
   "source": [
    "fig, _ = test_GWR(fake_flowers2, categories2)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that when the categories start to overlap, the network creates more nodes than the number of categories to try to be more precise, but in spite of that, the performance is still affected. Nevertheless, the resulting nodes are are a good approximation of the input space distribution, so it shows that the algorithm is efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"iris\"></a>5. A real dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't conclude on the efficiency of the algorithm without first trying it on the Iris dataset, so let's do it :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes : 3\n",
      "Train error : 10.0%\n",
      "Generalization error : 10.0%\n",
      "Confusion matrix :\n",
      " [[17  0  0]\n",
      " [ 0 15  1]\n",
      " [ 0  4 13]]\n"
     ]
    }
   ],
   "source": [
    "from dataset.load_iris import get_dataset\n",
    "\n",
    "flowers, iris_labels = get_dataset()\n",
    "fig, _ = test_GWR(flowers, iris_labels)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As predicted, the GWR creates 3 nodes that each correspond to a category of flowers. We can also see that the algorithm doesn't make any mistake on the prediction of the category 1 (which is quite different from the two other categories), but mix up a few flowers in the two categories that are overlapping.\n",
    "\n",
    "**NB :** the iris flowers are represented by a 4D vector, the 3D representation is simply a projection of the flowers in the 3D space, so some information is lost in the process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. <a id=part3></a>Overcoming catastrophic forgetting ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important thing we need to check is the assertion that neurogenesis can overcome catastrophic forgetting. For that, let's first generate 5 categories of flowers and let's train a network on only 4 of these categories :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes : 4\n",
      "Train error : 2.25%\n",
      "Generalization error : 2.0%\n",
      "Confusion matrix :\n",
      " [[47  0  0  0]\n",
      " [ 0 47  1  0]\n",
      " [ 0  0 44  0]\n",
      " [ 1  0  2 58]]\n"
     ]
    }
   ],
   "source": [
    "all_flowers, all_categories, all_centers = generate_dataset(nb_points=750, nb_labels=5, var=0.13)\n",
    "not_last_category, last_category = (all_categories != 4), (all_categories == 4)\n",
    "fake_flowers_not5, fake_flowers_5 = all_flowers[not_last_category], all_flowers[last_category]\n",
    "categories_not5, categories_5 = all_categories[not_last_category], all_categories[last_category]\n",
    "centers_not5, centers_5 = all_centers[:-1], all_centers[-1]\n",
    "\n",
    "plot_dataset(fake_flowers_not5, categories_not5, centers_not5)\n",
    "\n",
    "network = test_GWR(fake_flowers_not5, categories_not5, disp_anim=False, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train the network just on the last category :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes : 5\n",
      "Train error : 7.0%\n",
      "Generalization error : 8.0%\n",
      "Confusion matrix :\n",
      " [[ 0  0  0  0]\n",
      " [ 0  0  0  0]\n",
      " [ 0  0  0  0]\n",
      " [ 1  1  2 46]]\n"
     ]
    }
   ],
   "source": [
    "fig, _ = test_GWR(fake_flowers_5, categories_5, network=network)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that immediatly at the start of the training, as the data are quite different from the learned nodes of the network, the algorithm creates a new node in the area of the 5th category of flowers. So after that, the other nodes aren't really affected by the learning process and only the new node is learning the distribution of the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generalization error : 4.8%\n",
      "Confusion matrix :\n",
      " [[149   0   0   1   0]\n",
      " [  0 147   3   0   0]\n",
      " [  2   7 140   1   0]\n",
      " [  8   0   3 139   0]\n",
      " [  0   1   2   8 139]]\n"
     ]
    }
   ],
   "source": [
    "test_err  = 100 - (network.predict(all_flowers) == all_categories).sum() / (all_categories.shape[0]) * 100\n",
    "print(f\"Generalization error : {test_err:.4}%\")\n",
    "\n",
    "print(\"Confusion matrix :\\n\", confusion_matrix(all_categories, network.predict(all_flowers)))\n",
    "\n",
    "plot_dataset(all_flowers, all_categories, np.array([node.weights for node in network.nodes]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that even after retraining the network on completely different data, it didn't forget what it had learn previously ! The catastrophic forgetting is indeed overcome *via* neurogenesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=part4></a>IV. Deep GWR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In their NIPS paper, Parisi *et al.* developped a more complex architecture based on the GWR networks.\n",
    "Their idea is, as in Deep Neural Networks, to use several layers of GWR to be able to reduce again the dimensionality of the input. The architecture is the following :\n",
    "* The first layer of the network is a GWR that takes an image as input and that learns to represent the input space with a set of nodes. So these nodes correspond to a discretized version of the input space, simpler and that might be more meaningful.\n",
    "* Then, a second GWR network receives as input the weight distribution of the Best Matching Unit of the first layer. So this second layer learns an even simpler representation of the input space as it learns a representation of its representation.\n",
    "* After as many layers as necessary, the last layer of the network is a labelized GWR (called Associative GWR or AGWR) that associates an input to a learned category\n",
    "\n",
    "Exactly as in the case of Deep Learning, the objective is to get finer and finer descriptions of the input space with the several GWR layers, so that the classification in the last layer is easier.\n",
    "\n",
    "The authors tested this Deep GWR architecture on three different datasets and they showed that in the three cases, the performances of DGWR were better than those of classic deep Neural Networks.\n",
    "On a classification task on ten categories, the DGWR proved to be more robust to the multi-class setup than classical architectures due to the neurogenesis property that allows the network to retain the previously learned characteristics.\n",
    "![](Results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=ccl></a>Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this paper, the authors developped a pretty old and unusual Neural Network architecture developped to reproduce the biological mecanism of neurogenesis. They were able to implement a complex deep network from this idea and they showed that its unique properties made this algorithm able to outperform classical DNN algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=biblio></a>Bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction of GWR :<br/>\n",
    "Marsland, S., Shapiro, J., and Nehmzow, U. (2002). [A self-organising network that grows when required](https://seat.massey.ac.nz/personal/s.r.marsland/PUBS/NN02.pdf). *Neural Networks*, 15(89):10411058\n",
    "\n",
    "Application on temporal data :<br/>\n",
    "Strickert, M., & Hammer, B. (2005). [Merge SOM for temporal data](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.84.4929&rep=rep1&type=pdf). *Neurocomputing*, 64.\n",
    "\n",
    "Description of multi-layers GWR :<br/>\n",
    "German I. Parisi, Jun Tani, Cornelius Weber, Stefan Wermter. (2017). [Lifelong learning of human actions with deep neural network self-organization](https://reader.elsevier.com/reader/sd/pii/S0893608017302034?token=FD1B498D2F74836AE5D21F2594B14FF4AD9159867FDB012776AFBE28FCE8F613D0ABF796BC84D1B52D0CC6F4F0E20539). *Neural Networks*, 96:137149.\n",
    "\n",
    "Investigating the role of neurogenesis and presenting results on a new dataset :<br/>\n",
    "German I. Parisi, Xu Ji, Stefan Wermter. (2018). [On the role of neurogenesis in overcomingcatastrophic forgetting](https://arxiv.org/abs/1811.02113). *arXiv*:1811.02113"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
